{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project runs production level code to create a data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, DateType, TimestampType\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import psycopg2\n",
    "\n",
    "# Set defaults\n",
    "pd.set_option(\"max_rows\", 1000)\n",
    "pd.set_option(\"max_columns\", 1000)\n",
    "\n",
    "# Build spark session\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "                    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_files = ['../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat']\n",
    "\n",
    "def load_data(spark_files):\n",
    "    '''\n",
    "    Input: spark_files is a list of file names for i94 data\n",
    "    \n",
    "    Output: dataframes for i94 data, city data, and airport data\n",
    "    '''\n",
    "\n",
    "    # Read only 1 file of the immigration dataset to keep the size easy to work with\n",
    "    # Only reading 1k rows to preserve memory space\n",
    "    im_df = spark.read.format('com.github.saurfang.sas.spark').load(spark_files,forceLowercaseNames=True).limit(1000)\n",
    "    im_df = im_df.toPandas()\n",
    "    \n",
    "    # For CSV files:\n",
    "    # Only 2 and unchanging so no need for a complicated loop\n",
    "    airport_df = pd.read_csv('airport-codes_csv.csv', nrows=1000)\n",
    "    city_df = pd.read_csv('us-cities-demographics.csv', delimiter=';', nrows=1000)\n",
    "    \n",
    "    return im_df, airport_df, city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def __clean_col_names(df):\n",
    "    '''\n",
    "    Input: dataframe whose column names should be standardized and cleaned\n",
    "    Output: dataframe with cleaned column names\n",
    "    '''\n",
    "    cols = list(df.columns)\n",
    "    cols = [x.lower().replace(' ','_') for x in cols]\n",
    "\n",
    "    df.columns = cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Prep and Clean Data\n",
    "\n",
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prep_clean_im_df(im_df):\n",
    "    '''\n",
    "    Input:  Immigration data dataframe\n",
    "    Output: Cleaned and prepped dataframe\n",
    "    '''\n",
    "    # Update dtypes\n",
    "    im_df['cicid'] = im_df['cicid'].astype(object)\n",
    "    im_df['arrdate'] = pd.to_datetime(im_df['arrdate']).dt.date\n",
    "    im_df['depdate'] = pd.to_datetime(im_df['depdate']).dt.date\n",
    "    im_df['dtadfile'] = pd.to_datetime(im_df['dtadfile']).dt.date\n",
    "    im_df['i94yr']=im_df['i94yr'].astype(int)\n",
    "    im_df['i94mon']=im_df['i94mon'].astype(int)\n",
    "    im_df['i94cit']=im_df['i94cit'].astype(int)\n",
    "    im_df['i94res']=im_df['i94res'].astype(int)\n",
    "    #im_df['i94mode']=im_df['i94mode'].astype(int)\n",
    "    im_df['i94bir']=im_df['i94bir'].astype(int)\n",
    "    im_df['i94visa']=im_df['i94visa'].astype(int)\n",
    "    im_df['count']=im_df['count'].astype(int)\n",
    "    im_df['biryear']=im_df['biryear'].astype(int)\n",
    "\n",
    "    # Replace invalid values, then update remaining data type\n",
    "    im_test = np.where(im_df['dtaddto']=='D/S', '01011900', im_df['dtaddto'])\n",
    "    im_df['dtaddto'] = im_test\n",
    "    im_df['dtaddto'] = pd.to_datetime(im_df['dtaddto'], format = '%m%d%Y').dt.date\n",
    "    \n",
    "    # Drop all blank columns and rename as needed\n",
    "    im_df = im_df.drop(columns = ['occup', 'insnum']).rename(columns={'i94bir': 'age'}, inplace=True)\n",
    "\n",
    "    # Clean column names\n",
    "    im_df = __clean_col_names(im_df)\n",
    "\n",
    "    return im_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prep_clean_city(city_df):\n",
    "    '''\n",
    "    Input:  Geo data dataframe\n",
    "    Output: Cleaned and prepped dataframe\n",
    "    '''\n",
    "    # Clean column names\n",
    "    city_df = __clean_col_names(city_df)\n",
    "    \n",
    "    return city_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prep_clean_airport(airport_df):\n",
    "    '''\n",
    "    Input:  Airport data dataframe\n",
    "    Output: Cleaned and prepped dataframe\n",
    "    '''\n",
    "    airport_df['airline'] = airport_df['local_code'].str[2:]\n",
    "    airport_df[['latitude', 'longitude']] = airport_df['coordinates'].str.split(\",\", expand = True) \\\n",
    "                                                            .rename(columns = {0: 'latitude', 1: 'longitude'})\n",
    "    airport_df = airport_df.drop(columns = ['coordinates', 'continent', 'iata_code'])\n",
    "    \n",
    "    # Clean column names\n",
    "    airport_df = __clean_col_names(airport_df)\n",
    "    \n",
    "    return airport_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Date Data\n",
    "\n",
    "##### Create dataframe to be used for date and time dimension table by:\n",
    "1. Create master date column to match date fields on\n",
    "2. Create components from each date to be stored as separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# adapted from here: https://stackoverflow.com/questions/34898525/generate-list-of-months-between-interval-in-python\n",
    "dates = [\"1900-01-01\", \"2020-01-01\"]\n",
    "\n",
    "\n",
    "def __get_all_dates(dates):\n",
    "    '''\n",
    "    Input:  List of two dates, first as start date and second as end date\n",
    "    Output: Ordered dictionary of all dates at the day level between the start and end date, inclusive\n",
    "    '''\n",
    "    # format dates provided, assign them start and end signifiers\n",
    "    start, end = [datetime.strptime(_, \"%Y-%m-%d\") for _ in dates]\n",
    "    # return ordered dictionary of all days in between the start and end dates\n",
    "    return OrderedDict(((start + timedelta(_)) \\\n",
    "                        .strftime(r\"%Y-%m-%d\"), None) for _ in range((end - start).days)).keys()\n",
    "\n",
    "\n",
    "def prep_clean_dates(dates):\n",
    "    '''\n",
    "    Input:  Ordered dict of all dates from the last step\n",
    "    Output: Dataframe with all date part components as separate columns for each date in the list\n",
    "    '''\n",
    "    dates_df = __get_all_dates(dates)\n",
    "    dates_df = pd.DataFrame(list(dates_df)).rename(columns = {0: 'dates_master'})\n",
    "    dates_df['dates_master'] = pd.to_datetime(dates_df['dates_master'])\n",
    "    dates_df['month'] = dates_df['dates_master'].dt.month\n",
    "    dates_df['day'] = dates_df['dates_master'].dt.day\n",
    "    dates_df['year'] = dates_df['dates_master'].dt.year\n",
    "    dates_df['weekday'] = dates_df['dates_master'].dt.weekday\n",
    "    dates_df['week'] = dates_df['dates_master'].dt.week\n",
    "    dates_df['dates_master'] = (dates_df['dates_master'].dt.date).astype(object)\n",
    "\n",
    "    return dates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Convert to Spark and write to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_fact_im(im_df):\n",
    "    '''\n",
    "    Input: Pandas immigration dataframe\n",
    "    Output: None.  Spark immigration dataframe is written to spark table\n",
    "    '''\n",
    "    # Create the conceptual fact table schema in Spark\n",
    "    im_Schema = StructType([StructField(\"cicid\", StringType(), True), \\\n",
    "                            StructField(\"i94yr\", IntegerType(), True), \\\n",
    "                            StructField(\"i94mon\", IntegerType(), True), \\\n",
    "                            StructField(\"i94cit\", IntegerType(), True), \\\n",
    "                            StructField(\"i94res\", StringType(), True), \\\n",
    "                            StructField(\"i94port\", StringType(), True), \\\n",
    "                            StructField(\"arrdate\", DateType(), True), \\\n",
    "                            StructField(\"i94mode\", StringType(), True), \\\n",
    "                            StructField(\"i94addr\", StringType(), True), \\\n",
    "                            StructField(\"depdate\", StringType(), True), \\\n",
    "                            StructField(\"i94bir\", IntegerType(), True), \\\n",
    "                            StructField(\"i94visa\", IntegerType(), True), \\\n",
    "                            StructField(\"count\", IntegerType(), True), \\\n",
    "                            StructField(\"dtadfile\", StringType(), True), \\\n",
    "                            StructField(\"visapost\", StringType(), True), \\\n",
    "                            StructField(\"entdepa\", StringType(), True), \\\n",
    "                            StructField(\"entdepd\", StringType(), True), \\\n",
    "                            StructField(\"entdepu\", StringType(), True), \\\n",
    "                            StructField(\"matflag\", StringType(), True), \\\n",
    "                            StructField(\"biryear\", IntegerType(), True), \\\n",
    "                            StructField(\"dtaddto\", DateType(), True), \\\n",
    "                            StructField(\"gender\", StringType(), True), \\\n",
    "                            StructField(\"airline\", StringType(), True), \\\n",
    "                            StructField(\"admnum\", DoubleType(), True), \\\n",
    "                            StructField(\"fltno\", StringType(), True), \\\n",
    "                            StructField(\"visatype\", StringType(), True)])\n",
    "\n",
    "    spark_im_df = spark.createDataFrame(im_df,schema=im_Schema)\n",
    "    #spark_im_df.write.mode(\"append\").parquet(\"/results/fact_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_date(dates_df):\n",
    "    '''\n",
    "    Input: Pandas dates dataframe\n",
    "    Output: None.  Spark dataframe is written to spark table\n",
    "    '''\n",
    "\n",
    "    # Create the conceptual date dimension table\n",
    "    dates_Schema = StructType([StructField(\"dates_master\", StringType(), True), \\\n",
    "                            StructField(\"month\", IntegerType(), True), \\\n",
    "                            StructField(\"day\", IntegerType(), True), \\\n",
    "                            StructField(\"year\", IntegerType(), True), \\\n",
    "                            StructField(\"weekday\", StringType(), True), \\\n",
    "                            StructField(\"week\", StringType(), True)])\n",
    "\n",
    "    spark_dates_df = spark.createDataFrame(dates_df,schema=dates_Schema)\n",
    "\n",
    "    #write table\n",
    "    spark_dates_df.write.mode(\"append\").parquet(\"/results/dim_dates.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_airport(airport_df):\n",
    "    '''\n",
    "    Input: Pandas airports dataframe\n",
    "    Output: None.  Spark dataframe is written to spark table\n",
    "    '''\n",
    "\n",
    "    # Create the conceptual airlines dimension table\n",
    "    airport_Schema = StructType([StructField(\"ident\", StringType(), True), \\\n",
    "                            StructField(\"type\", StringType(), True), \\\n",
    "                            StructField(\"name\", StringType(), True), \\\n",
    "                            StructField(\"elevation_ft\", DoubleType(), True), \\\n",
    "                            StructField(\"iso_country\", StringType(), True), \\\n",
    "                            StructField(\"iso_region\", StringType(), True),\\\n",
    "                            StructField(\"municipality\", StringType(), True), \\\n",
    "                            StructField(\"gps_code\", StringType(), True), \\\n",
    "                            StructField(\"local_code\", StringType(), True), \\\n",
    "                            StructField(\"latitude\", StringType(), True), \\\n",
    "                            StructField(\"longitude\", StringType(), True), \\\n",
    "                            StructField(\"airline\", StringType(), True)])\n",
    "\n",
    "    spark_airport_df = spark.createDataFrame(airport_df,schema=airport_Schema)\n",
    "\n",
    "    spark_airport_df.write.mode(\"append\").parquet(\"/results/dim_airport.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_geo(city_df):\n",
    "    '''\n",
    "    Input: Pandas geography and population dataframe\n",
    "    Output: None.  Spark dataframe is written to spark table\n",
    "    '''\n",
    "    geo_Schema = StructType([StructField(\"city\", StringType(), True), \\\n",
    "                        StructField(\"state\", StringType(), True), \\\n",
    "                        StructField(\"median_age\", DoubleType(), True), \\\n",
    "                        StructField(\"male_population\", DoubleType(), True), \\\n",
    "                        StructField(\"female_population\", DoubleType(), True), \\\n",
    "                        StructField(\"total_population\", IntegerType(), True), \\\n",
    "                        StructField(\"number_of_veterans\", DoubleType(), True), \\\n",
    "                        StructField(\"foreign_born\", DoubleType(), True), \\\n",
    "                        StructField(\"average_household_size\", DoubleType(), True), \\\n",
    "                        StructField(\"state_code\", StringType(), True), \\\n",
    "                        StructField(\"race\", StringType(), True), \\\n",
    "                        StructField(\"count\", IntegerType(), True)])\n",
    "\n",
    "    spark_geo_df = spark.createDataFrame(city_df,schema=geo_Schema)\n",
    "\n",
    "    spark_geo_df.write.mode(\"append\").parquet(\"/results/dim_geo.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check(df):\n",
    "    '''\n",
    "    Input: Dataframe\n",
    "    Output: Outcome of data quality check\n",
    "    '''\n",
    "    if df.count() == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(df))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(df, df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_pipeline(spark_files, dates):\n",
    "    load_data(spark_files)\n",
    "    prep_clean_im_df(im_df)\n",
    "    prep_clean_city(city_df)\n",
    "    prep_clean_airport(airport_df)\n",
    "    prep_clean_dates(dates)\n",
    "    create_fact_im(im_df)\n",
    "    create_dim_date(dates_df)\n",
    "    create_dim_airport(airport_df)\n",
    "    create_dim_geo(city_df)\n",
    "    quality_check(im_df)\n",
    "    quality_check(city_df)\n",
    "    quality_check(dates_df)\n",
    "    quality_check(airport_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration Fact Table\n",
    "- cicid: ID number for each applicant\n",
    "- i94yr: 4 digit year of I9\n",
    "- i94mon: Numeric month of I9\n",
    "- i94cit: 3 digit code of original citizenship country\n",
    "- i94res: 3 digit code of original residence country\n",
    "- i94port: 3 digit code of US city upon arrival\n",
    "- arrdate: Date arrived to US\n",
    "- i94mode: 1 digit travel code indicating travel type (air, land, sea)\n",
    "- i94addr: State on i94 visa, 2 digit code\n",
    "- depdate: Departure date\n",
    "- i94bir: Age\n",
    "- i94visa: Reason requesting i94 visa (1-3, indicating business, pleasure, or student)\n",
    "- count: 1 for all records; used for quick counts\n",
    "- dtadfile: Date added to file\n",
    "- visapost: Department of State office issuing the visa\n",
    "- entdepa: Arrival flag, either admitted or not\n",
    "- entdepd: Departure flag (indcating departed, lost I-94 or is deceased)\n",
    "- entdepu: Update flag (indicating apprehended, overstayed, adjusted to perm residence)\n",
    "- matflag: Match flag (match of arrival and departure records)\n",
    "- biryear: 4 digit year where born\n",
    "- dtaddto: Date allowed to stay in US until\n",
    "- gender: Gender of i94 visa holder\n",
    "- airline: Airline used to arrive in U.S.\n",
    "- admnum: Admission number\n",
    "- fltno: Flight number of Airline used to arrive in U.S.\n",
    "- visatype: type of i94 visa (letter + number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimension Airport Table\n",
    "- ident: Alphanumeric code identifying airport\n",
    "- type: Type of airport (e.g. helipad, small airport, closed)\n",
    "- name: Name of airport\n",
    "- elevation_ft: Elevation where airport is located, in feet\n",
    "- iso_country: Country code of airport\n",
    "- iso_region: Region code of airport, combination of 2 digit country code and 2 digit state abbreviation\n",
    "- municipality: Locality / city where airport is located\n",
    "- gps_code: Alphanumeric code identifying the airport\n",
    "- local_code: Alphanumeric code identifying the airport\n",
    "- airline: Last 2 digits of local code since most are padded with leading zeros; to facilitate joins on city code\n",
    "- latitude: Latitude of airport\n",
    "- longitude: Longitude of airport\n",
    "\n",
    "**Dropped**: coordinates, continent, and iata code columns.  IATA code would be highly useful for matching out to other datasets \n",
    "but unfortunately all codes were missing in this dataset.  The continent column was the same value since all destination airports are in the same continent.  Coordinates was dropped because it was separated into latitude and longitude columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimension Geo Table\n",
    "- city: city\n",
    "- state: state (no abbreviation)\n",
    "- median_age: median age for the combination of city, state, population, and race slice the row represents\n",
    "- male_population: male population for the combination of city, state, population, and race slice the row represents\n",
    "- female_population: female population for the combination of city, state, population, and race slice the row represents\n",
    "- total_population: total population for the combination of city, state, population, and race slice the row represents\n",
    "- number_of_veterans: number of veterans for the combination of city, state, population, and race slice the row represents\n",
    "- foreign-born: number of foreign born residents for the combination of city, state, population, and race slice the row represents\n",
    "- average_household_size: typical number of people living in one home for the combination of city, state, population, and race slice the row represents\n",
    "- state_code: 2 digit state abbreviation\n",
    "- race: Race of the population data represented in the city-state row\n",
    "- count: Number of i94 visas for that city state\n",
    "\n",
    "**Dropped:** No columns dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dimension Date Table\n",
    "- dates_master: YYYY-MM-DD formatted list of all days between 1900-01-01 and 2020-01-01\n",
    "- month: Month of date in dates_master\n",
    "- day: Day of date in dates_master\n",
    "- year: Year of date in dates_master\n",
    "- weekday: Day of week (string) of date in dates_master\n",
    "- week: Week ID for the date in dates_master\n",
    "\n",
    "**Dropped:** No columns dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Rational for choice of tools and technologies:  \n",
    "- A data warehouse was used rather than a data lake because this project had a pre-defined purpose of preparing immigration i94 data for predictive modeling.  Data lakes are more commonly used for vast pools of data with no defined structure.  Cleaning and dropping fields happens after extracting the data from the lake, not prior to creating the lake.  The warehouse structure allows for dropping all blank fields.\n",
    "- Spark was used given the size of the i94 data.  For purpose of code development, a single file was used.  In production, the entire dataset should be processed and stored.  Spark's parallelization and distributed computing capabilities make it the ideal tool for working with data of this scale.\n",
    "- Python was also used for quick analysis and quality checks.  In the development phase especially, this was ideal for getting to the answer that drives business value quickly.\n",
    "- A STAR schema was used for the tables to facilitate easy analysis and logical, structured data organization.  The fact table is the i94 immigration data.  The dimensions tables include geographic data, airport code data, and date data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Update schedule:\n",
    "- i94 data should be updated as frequently as each new refresh is available from the government.  This data comprises the fact table, which is the heart of the data warehouse.  It is also the central piece around which all modeling questions are based.  Given this key role, it needs to be as complete and current as possible as consistently as possible.\n",
    "- Geographic data and airport data are unlikely to change significantly as often.  Geographic data does contain population information, so an annual refresh would be ideal.  Alternately, if an annual refresh is not possible, updates should use the latest U.S. census data and mirror the census data update schedule.  Airport data should be monitored annually as well, especially for closures of smaller locations.  This is the data that is most likely to stay consistent so this refresh can take lower priority compared to i94 data and geographic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Contingency Scenario Planning:\n",
    "- If the data was increased 100x, I recommend continuing to stick with the Redshift platform but adapting the number of clusters and workers to optimize job performance.\n",
    "- If a dashboard must be updated by 7am daily, I recommend scheduling this job to run autmoatically overnight.  If the data quality check is not passed, a set of rules should be set up so that the dashboard is not populated and the same data from the prior day remains on the dashboard.  Slightly old data is better than bad data!  The analyst or data scientist could then look at the failures upon arriving to the office that morning.  In the event that delay is not feasible for the business need and this fits with the data availability / refresh schedule, I recommend running the job at the end of the day the day prior (say 4pm on Monday for the Tuesday 7am refresh).  This would allow someone to manually review any flags thrown during the ingest and update process.  Then, a separate job could just refresh the GUI prior to 7am, essentially separating the data ingest and check process from the dashboard update process.\n",
    "- If the database needs accessed by 100+ people, I recommend using Redshift's autoscaling feature to accommodate sudden, unanticipated changes in access and querying.  Redshift also offers good read performance, so would be a better fit than Apache Cassandra, which offers better write performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
